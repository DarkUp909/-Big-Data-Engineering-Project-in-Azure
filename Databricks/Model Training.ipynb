{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27d6cc18-0b6d-4aef-acbc-9a4666f1f566",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Use machine learning -- Sentiment analysis method to train a machine learning model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cc41344a-a7bf-479d-a7f9-937dac0fa53d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### STEP 0. Loading the data into dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e183e278-1bf0-486f-b9fd-7096100484a8",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Creating a spark session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .appName(\"Table Loading\")\n",
    "         .getOrCreate())\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bce54f6d-957d-4744-bdbb-39722cf796d1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###### 0.1 Creating the Posts dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52d573ec-6dc1-441f-8a63-9bff498d6122",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# File location -- recall our mount storage workshop, data was mounted into '/mnt/BDProject'\n",
    "file_location = \"/mnt/BDProject/ml_training/Posts/*\"\n",
    "\n",
    "posts = spark.read \\\n",
    "  .parquet(file_location)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59491213-3690-447c-b73e-4c5c893b15f7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###### 0.2 Creating the posttypes dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98045c80-0cc9-4f66-8bef-ec81b6d6e0b9",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Creating the schema for posttypes table\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "PT_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"Type\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "938bfbf4-41ff-4bfb-94ea-17fe77ebefcd",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Creating the posttypes dataframe\n",
    "file_location = \"/mnt/BDProject/ml_training/PostTypes.txt\"\n",
    "\n",
    "postType = (spark.read\n",
    "  .option(\"header\", \"true\")\n",
    "  .option(\"sep\", \",\")\n",
    "  .schema(PT_schema)\n",
    "  .csv(file_location))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b8d8562f-9226-4813-8f1b-43c3b12d837d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###### 0.3 Creating the users dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e55024f5-a5e5-4469-b65f-8013877b03f3",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Creating the schema for the users table\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "users_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"Age\", IntegerType(), True),\n",
    "    StructField(\"CreationDate\", DateType(), True),\n",
    "    StructField(\"DisplayName\", StringType(), True),\n",
    "    StructField(\"DownVotes\", IntegerType(), True),\n",
    "    StructField(\"EmailHash\", StringType(), True),\n",
    "    StructField(\"Location\", StringType(), True),\n",
    "    StructField(\"Reputation\", IntegerType(), True),\n",
    "    StructField(\"UpVotes\", IntegerType(), True),\n",
    "    StructField(\"Views\", IntegerType(), True),\n",
    "    StructField(\"WebsiteUrl\", StringType(), True),\n",
    "    StructField(\"AccountId\", IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7551bf4-799d-40f8-9646-717eb943bb52",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Creating the users dataframe\n",
    "file_location = \"/mnt/BDProject/ml_training/users.csv\"\n",
    "\n",
    "users = (spark.read\n",
    "  .option(\"header\", \"true\")\n",
    "  .option(\"sep\", \",\")\n",
    "  .schema(users_schema)\n",
    "  .csv(file_location))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31dfda97-8579-4a35-87c0-b3194a2c18dc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###### 0.4. Saving the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ce5ab89-6e13-481b-a9ab-27d530e86992",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Save the 3 tables to databricks local file system\n",
    "posts.write.mode('overwrite').parquet(\"/tmp/project/posts.parquet\")\n",
    "postType.write.mode('overwrite').parquet(\"/tmp/project/PostType.parquet\")\n",
    "users.write.mode('overwrite').parquet(\"/tmp/project/user.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d99aceec-69dd-4d9a-80b3-2083b7fc969c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### STEP 1. Join tables and filter data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14f0aef6-feab-4eee-bcdb-acabb56918fa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###### 1.1 Prepare necessary libraries and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d21c53c-74f4-49e2-9457-234dd1bc3c3f",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries and functions\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split, translate, trim, explode, regexp_replace, col, lower, to_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd3fa37b-1a75-4618-91f9-65784e3d04d0",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Creating Spark Session\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .appName(\"ML Model\")\n",
    "         .getOrCreate())\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "011bccb9-e39b-4415-b7b9-370767fbc6b2",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Read in the tables\n",
    "\n",
    "posts = spark.read.parquet(\"/tmp/project/posts.parquet\")\n",
    "postType = spark.read.parquet(\"/tmp/project/PostType.parquet\")\n",
    "Users = spark.read.parquet(\"/tmp/project/user.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba923f0e-32f6-4dfc-b435-c495dda729dd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###### 1.2 Join the tables Posts and postTypes by it post type id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "261f60bb-159f-4936-8673-21ce1b865aef",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Use Posts and posttypes to train the model.\n",
    "\n",
    "df = posts.join(postType, posts.PostTypeId == postType.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3861f425-e3cf-4724-9c64-95855fc3770f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###### 1.3 Filter the data\n",
    "\n",
    "In the posttypes table, there is a column called Type which indicates if the posts is a question or an answer. We only need the 'question' entires. For these 'Question' rows, we will run machine learning model on the join the 'Body' column of the 'Posts' table. To tell what topic this post is about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e4f884c-1809-4fc1-9dad-2e3d36ab2010",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Filter the dataframe to only include questions\n",
    "df = df.filter(col(\"Type\") == \"Question\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d7d1926-7c52-45eb-9a93-ca52eb78dbd8",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Formatting the 'Body' and `Tag` columns for machine learning training\n",
    "df = (df.withColumn('Body', regexp_replace(df.Body, r'<.*?>', '')) # Transforming HTML code to strings\n",
    "      .withColumn(\"Tags\", split(trim(translate(col(\"Tags\"), \"<>\", \" \")), \" \")) # Making a list of the tags\n",
    "      .withColumn(\"Date\", to_date(\"CreationDate\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f54acfa-8c48-4d30-9714-28331b4a34e2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###### 1.4 Create a checkpoint to save the dataframe to file only contain the Body, Tag, and date we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "52d2fe8f-c457-4194-b4c7-a78fb89083f5",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df = df.select(col(\"Body\").alias(\"text\"), col(\"Tags\"), col(\"Date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bb51e57-7d84-43c7-931f-525ff18c4a4c",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Producing the tags as individual tags instead of an array\n",
    "# This is duplicating the posts for each possible tag\n",
    "df = df.select(\"text\", explode(\"Tags\").alias(\"tags\"), col(\"Date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc2f3c93-ed9d-495b-b90f-3fec722a8ec4",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# saving the file as a checkpoint (in case the cluster gets terminated)\n",
    "\n",
    "df.write.mode('overwrite').parquet(\"/tmp/project.df.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85df6b46-df3a-4944-b1a0-9b9cdc55430a",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Saving the dataframe to memory for repetitive use\n",
    "df.cache()\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e87752ae-32b4-481b-9009-e3716556b5f2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### STEP 2. Based on the above dataframe, prepare data for machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7399f604-fc75-45c8-aa8b-46588d58a8d3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###### 2.1. Text Cleaning Preprocessing\n",
    "\n",
    "pyspark.sql.functions.regexp_replace is used to process the text\n",
    "\n",
    "1. Remove URLs\n",
    "2. Remove special characters\n",
    "3. Substituting multiple spaces with single space\n",
    "4. Lowercase all text\n",
    "5. Trim the leading/trailing whitespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "448351ed-0eb3-4b36-aa22-577e6917bab0",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Preprocessing the data\n",
    "cleaned = df.withColumn('text', regexp_replace('text', r\"http\\S+\", \"\")) \\\n",
    "                    .withColumn('text', regexp_replace('text', r\"[^a-zA-z]\", \" \")) \\\n",
    "                    .withColumn('text', regexp_replace('text', r\"\\s+\", \" \")) \\\n",
    "                    .withColumn('text', lower('text')) \\\n",
    "                    .withColumn('text', trim('text')) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b83346d2-3ac1-40d6-ad9f-133ae053a7c8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### STEP 3. Machine Learning Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d5471e0-c615-478d-8713-4bf2850fe054",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###### 3.1 Feature Transformer\n",
    "\n",
    "3.1.1 Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5fd2e78-a4b5-44d5-a754-8159b0ef9d1c",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\n",
    "tokenized = tokenizer.transform(cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c3fc81f2-0076-4831-8991-5de1988a18d7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "3.1.2 Stopword Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75fe24e8-d472-4347-881b-f8fbf247a2c1",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "stopword_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered\")\n",
    "stopword = stopword_remover.transform(tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c056418-70e6-4b9a-985e-aa231b8477c8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "3.1.3 CountVectorizer (TF - Term Frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fbb4747c-ea7b-4c08-ab02-20ab647b36ea",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "CountVectorizer: converting a collection of text into a matrix of token counts.\n",
    "\n",
    "vocabSize=2**16: the vocabulary size or the maximum number of unique words or tokens that the system can handle is set to 65,536"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8d1077e-f955-4de7-8b07-0c54c7ce1eda",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(vocabSize=2**16, inputCol=\"filtered\", outputCol='cv')\n",
    "cv_model = cv.fit(stopword)\n",
    "text_cv = cv_model.transform(stopword)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4df20ec8-bb5b-4cc2-b527-23f7c8b2a9ca",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "3.1.4 TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50270094-7af9-420a-872d-0b5a1ca1b30f",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "\n",
    "idf = IDF(inputCol='cv', outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "idf_model = idf.fit(text_cv)\n",
    "text_idf = idf_model.transform(text_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a34f8939-38c1-444d-81e3-a0d191e21862",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###### 3.2 Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8f8d9ad-b55c-45af-b9e6-02cb8e95c3d5",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "label_encoder = StringIndexer(inputCol = \"tags\", outputCol = \"label\")\n",
    "le_model = label_encoder.fit(text_idf)\n",
    "final = le_model.transform(text_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5046423-5f56-4995-ba01-da93d0feb7c4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###### 3.3 Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f539c1a0-d193-4922-95ce-fa37d3f04621",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(maxIter=100)\n",
    "\n",
    "lr_model = lr.fit(final)\n",
    "\n",
    "predictions = lr_model.transform(final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "080b7d6d-73f2-450a-923d-61cdb92a700c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###### 3.4 Model Evalution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9783f29b-a4c2-4b8c-8195-71821462b893",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "roc_auc = evaluator.evaluate(predictions)\n",
    "accuracy = predictions.filter(predictions.label == predictions.prediction).count() / float(predictions.count())\n",
    "\n",
    "print(\"Accuracy Score: {0:.4f}\".format(accuracy))\n",
    "print(\"ROC-AUC: {0:.4f}\".format(roc_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23bb0a6d-f66e-434b-bbb5-aef6ed69d3c7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###### 3.5 Create a Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cf5ab44-f87d-4e48-a8c6-58017a9f2f50",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Importing all the libraries\n",
    "from pyspark.sql.functions import split, translate, trim, explode, regexp_replace, col, lower\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, StringIndexer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Preparing the data\n",
    "# Step 1: Creating the joined table\n",
    "df = posts.join(postType, posts.PostTypeId == postType.id)\n",
    "# Step 2: Selecting only Question posts\n",
    "df = df.filter(col(\"Type\") == \"Question\")\n",
    "# Step 3: Formatting the raw data\n",
    "df = (df.withColumn('Body', regexp_replace(df.Body, r'<.*?>', ''))\n",
    "      .withColumn(\"Tags\", split(trim(translate(col(\"Tags\"), \"<>\", \" \")), \" \"))\n",
    "      .withColumn(\"Date\", to_date(\"CreationDate\"))\n",
    ")\n",
    "# Step 4: Selecting the columns\n",
    "df = df.select(col(\"Body\").alias(\"text\"), col(\"Tags\"), col(\"Date\"))\n",
    "# Step 5: Getting the tags\n",
    "df = df.select(\"text\", explode(\"Tags\").alias(\"tags\"), col(\"Date\"))\n",
    "# Step 6: Clean the text\n",
    "cleaned = df.withColumn('text', regexp_replace('text', r\"http\\S+\", \"\")) \\\n",
    "                    .withColumn('text', regexp_replace('text', r\"[^a-zA-z]\", \" \")) \\\n",
    "                    .withColumn('text', regexp_replace('text', r\"\\s+\", \" \")) \\\n",
    "                    .withColumn('text', lower('text')) \\\n",
    "                    .withColumn('text', trim('text')) \n",
    "\n",
    "# Machine Learning\n",
    "# Step 1: Train Test Split\n",
    "train, test = cleaned.randomSplit([0.9, 0.1], seed=20200819)\n",
    "# Step 2: Initializing the transfomers\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\n",
    "stopword_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered\")\n",
    "cv = CountVectorizer(vocabSize=2**16, inputCol=\"filtered\", outputCol='cv')\n",
    "idf = IDF(inputCol='cv', outputCol=\"features\", minDocFreq=5)\n",
    "label_encoder = StringIndexer(inputCol = \"tags\", outputCol = \"label\")\n",
    "lr = LogisticRegression(maxIter=100)\n",
    "# Step 3: Creating the pipeline\n",
    "pipeline = Pipeline(stages=[tokenizer, stopword_remover, cv, idf, label_encoder, lr])\n",
    "# Step 4: Fitting and transforming (predicting) using the pipeline\n",
    "pipeline_model = pipeline.fit(train)\n",
    "predictions = pipeline_model.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f86627c7-c907-4446-9898-afd2cce00661",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### STEP 4. Save the Model file to Azure storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c99ace2-345e-4bbc-9053-625e7dcbc875",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Saving model object to the /mnt/BDProject directory.\n",
    "pipeline_model.save('/mnt/BDProject/modelU')\n",
    "\n",
    "# Save the the String Indexer to decode the encoding. We need it in the future Sentiment Analysis.\n",
    "le_model.save('/mnt/BDProject/stringindexerU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb7ca9f8-6d04-4b4d-80b6-d9399297a8b9",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Review the directory\n",
    "display(dbutils.fs.ls(\"/mnt/BDProject/modelU\"))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Model Training",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
